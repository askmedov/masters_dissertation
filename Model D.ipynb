{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import matthews_corrcoef as matt\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading the data. Data has stock tickers in the first row and dates in the first column. Only trading days are used.\n",
    "\"\"\"\n",
    "df = pd.read_excel('C:/Users/FS_Askar_A/midcap.xlsx', index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_returns(dtf, tick, lead=5, lags=[1,2,3,4,5,7,10,15,20,30,50], tr=0.025):\n",
    "    \"\"\"\n",
    "    Calculates forward and lagged returns for a stock. Also creates target column\n",
    "    for returns over some threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - data with all stock returns\n",
    "    tick - ticker of stock \n",
    "    lead - forward return lead\n",
    "    lags - lagged returns\n",
    "    tr - threshold value for target variable\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame for one stock with its price, forward and lagged returns and target column.\n",
    "    \"\"\"\n",
    "    dtf = dtf[[tick]]\n",
    "    dtf['fwd'] = dtf[tick].shift(-lead) / dtf[tick]-1\n",
    "    for lag in lags:\n",
    "        name = 'ret'+str(lag)\n",
    "        dtf[name] = dtf[tick]/dtf[tick].shift(lag)-1\n",
    "    dtf['ycol'] = np.where(dtf['fwd'] >= tr, 1, 0)\n",
    "    return dtf.dropna()\n",
    "\n",
    "\n",
    "def my_tss(dtf, split, train_size=1000, test_size=100):\n",
    "    \"\"\"\n",
    "    My take on time series split. It separates dataframe into \"past\" and \n",
    "    \"future\" from a splitting point. Also removes extra columns. \n",
    "    \n",
    "    Parameters:\n",
    "    dtf - dataframe of a stock\n",
    "    split - splitting point in trading days\n",
    "    train_size=1000 - size of the training sample\n",
    "    test_size=100 - size of the testing sample\n",
    "    \n",
    "    Returns:\n",
    "    trainx - training features\n",
    "    trainy - training target column\n",
    "    testx - testing features\n",
    "    testy - testing target column\n",
    "    \n",
    "    Attention!\n",
    "    This function is not fool-proof - it doesn't check whether the size of \n",
    "    the train size is less than splitting point or if testing sample has\n",
    "    any values. This is done for speed purposes as it is called thousands of times.\n",
    "    \"\"\"\n",
    "    train_start = split - train_size\n",
    "    test_end = split + test_size\n",
    "    trainx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol']).iloc[train_start:split]\n",
    "    testx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol']).iloc[split:test_end]\n",
    "    trainy = dtf['ycol'].iloc[train_start:split]\n",
    "    testy = dtf['ycol'].iloc[split:test_end]\n",
    "    return trainx, testx, trainy, testy\n",
    "\n",
    "\n",
    "def integerize(d):\n",
    "    \"\"\"\n",
    "    Converts hyperparameter values into integers. This is a compensation of hyperopt's \n",
    "    problem where it feeds integer values in float form. I.e. 2.0 instead of 2.\n",
    "    \n",
    "    Parameters:\n",
    "    d - dictionary of hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary of hyperparameters with integers where required\n",
    "    \"\"\"\n",
    "    \n",
    "    int_params = [\n",
    "        'train_size',\n",
    "        'test_size',\n",
    "        'num_leaves',\n",
    "        'max_depth',\n",
    "        'n_estimators',\n",
    "        'min_child_samples',\n",
    "        'upto',\n",
    "        'ticks_to_use'\n",
    "    ]\n",
    "    \n",
    "    for k in d:\n",
    "        if k in int_params:\n",
    "            d[k] = int(d[k])\n",
    "            \n",
    "    return d\n",
    "\n",
    "\n",
    "def calc_ret(results):\n",
    "    \"\"\"\n",
    "    Calculation of return from predicted data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results - list of dataframes. Dataframes must have 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ===negative=== return for the whole period, across all the dataframes. \n",
    "                Return is negative because hyperopt minimizes a function.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigdf = pd.concat(results)\n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in bigdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        x+=1\n",
    "        \n",
    "    return 1-np.sum(subportf)/portf\n",
    "\n",
    "\n",
    "def check_params(params, upto, stock_num=20):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters. \n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    params - optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe with 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \"\"\"\n",
    "    params = integerize(params)\n",
    "    bigdf = pd.DataFrame()\n",
    "    \n",
    "    for tick in df.columns[:stock_num]:\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], upto+100, \n",
    "                                              train_size=params['train_size'])\n",
    "        \n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "        \n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "            \n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        \n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "        \n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "        \n",
    "    return bigdf\n",
    "\n",
    "\n",
    "def check_paramsEF(tick_params, upto):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters for every ticker separately. \n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    tick_params - dict of optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe with 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigdf = pd.DataFrame()\n",
    "    \n",
    "    for tick, params in tick_params.items():\n",
    "        \n",
    "        params = integerize(params)\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], upto+100, \n",
    "                                              train_size=params['train_size'])\n",
    "        \n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "        \n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "\n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        \n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "        \n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "        \n",
    "    return bigdf\n",
    "\n",
    "\n",
    "def get_est_series(dtf, com=0):\n",
    "    \"\"\"\n",
    "    Calculates total return over several out-of-cross-validation samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - dataframe with 'avg' column\n",
    "    com=0 - value of commissions for ==one== side of a trade in USD/share\n",
    "    \n",
    "    Returns:\n",
    "    portf_series - series with portfolio value at a point in time\n",
    "    \"\"\"\n",
    "    \n",
    "    comdf = dtf.drop(columns=['avg'])\n",
    "    for tick in dtf.columns[:-1]:\n",
    "        if df[tick].iloc[1600] >= 0:\n",
    "            comdf[tick] = (df[tick].shift(-5)-com) / (df[tick]+com)-1\n",
    "            comdf[tick] = np.where(dtf[tick].notna(), comdf[tick], np.nan)\n",
    "        else:\n",
    "            comdf[tick] = np.nan\n",
    "    comdf['avg'] = comdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    portf_series = []\n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in comdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        portf_series.append(subportf.copy())\n",
    "        x+=1\n",
    "    \n",
    "    portf_series = pd.DataFrame(portf_series, index=comdf.index)\n",
    "    portf_series = portf_series.shift(5)\n",
    "    portf_series = portf_series.fillna(20).sum(axis=1)\n",
    "    \n",
    "    return portf_series\n",
    "\n",
    "\n",
    "def get_res(params):\n",
    "    \n",
    "    mymodel_results = []\n",
    "\n",
    "    for tick in df.columns:\n",
    "\n",
    "        for split in range(1000, 2200, 100):\n",
    "            trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split)\n",
    "            my_model = LGBMClassifier(**params)\n",
    "            my_model.fit(trainx, trainy)\n",
    "            predy = my_model.predict(testx)\n",
    "            mymodel_results.append(matt(testy, predy))\n",
    "\n",
    "    return np.mean(mymodel_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tickers_dfs = {}\n",
    "\n",
    "for tick in tqdm_notebook(df.columns):\n",
    "    tickers_dfs[tick] = create_returns(df, tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_upto_modelD(params):\n",
    "    \"\"\"\n",
    "    Cross-validation function that takes does hyperparameters \n",
    "    validation upto some maximum split point and yields total\n",
    "    return as output (negative for hyperopt purposes)\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    params - dictionary with hyperparameters from hyperopt\n",
    "                and 'upto' key, that limits cross-validation in time\n",
    "                \n",
    "    Returns:\n",
    "    --------\n",
    "    average matt score for the cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    params = integerize(params)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                         max_depth=params['max_depth'],\n",
    "                                         learning_rate=params['learning_rate'],\n",
    "                                         n_estimators=params['n_estimators'],\n",
    "                                         min_child_samples=params['min_child_samples'])\n",
    "    \n",
    "    for split in range(params['upto']-500, params['upto'], 100):\n",
    "        \n",
    "        tempdf = pd.DataFrame()\n",
    "        \n",
    "        for tick in df.columns[:params['ticks_to_use']]:\n",
    "            trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split, \n",
    "                                                      train_size=params['train_size'])\n",
    "            \n",
    "            sw = np.where(trainy==0, params['sw'], 1)\n",
    "            \n",
    "            my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "            testx['pred'] = my_model.predict(testx)\n",
    "            tempdf['pred'] = testx['pred']\n",
    "            tempdf['fwd'] = tickers_dfs[tick]['fwd']\n",
    "            tempdf[tick] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        \n",
    "        results.append(tempdf)\n",
    "    \n",
    "    return -calc_ret(results)\n",
    "\n",
    "def bayes_opt_modelD(upto, ticks_to_use=20, eval_n=100):\n",
    "    \"\"\"\n",
    "    Hyperopt's Bayesian optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    upto - maximum split point for cross-validations\n",
    "    eval_n=100 - number of evaluations to do\n",
    "    \n",
    "    Returns:\n",
    "    tpe_best - best found hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    space_index = {\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 50, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 3, 8, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.05), np.log(0.2)),\n",
    "        'n_estimators': hp.quniform('n_estimators', 32, 512, 8),\n",
    "        'min_child_samples': hp.quniform('min_child_samples', 10, 50, 5),\n",
    "        'sw': hp.uniform('sw', 0.4, 0.7),\n",
    "        'train_size': hp.quniform('train_size', 400, 1000, 100),\n",
    "        'upto': hp.choice('upto', [upto]),\n",
    "        'ticks_to_use': ticks_to_use\n",
    "    }\n",
    "    \n",
    "    tpe_algo = tpe.suggest\n",
    "    tpe_trials = Trials()\n",
    "    tpe_best = fmin(fn=analyze_upto_modelD, space=space_index, algo=tpe_algo, \n",
    "                    trials=tpe_trials, max_evals=eval_n, verbose=True)\n",
    "    \n",
    "    return tpe_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating the overall matt score for Model D20\n",
    "First 20 stocks only.\n",
    "\"\"\"\n",
    "\n",
    "modelD20_returns = []\n",
    "for upto in tqdm_notebook(range(1500, 2100, 100)):\n",
    "\n",
    "    opt_params = bayes_opt_modelD(upto)\n",
    "    modelD20_returns.append(check_params(opt_params, upto))\n",
    "\n",
    "modelD20_ret = get_est_series(pd.concat(modelD20_returns))\n",
    "\n",
    "print(\"Average return for Model D20:\", \\\n",
    "      round(modelD20_ret.iloc[-1]-100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating the overall matt score for Model D317\n",
    "All 317 stocks.\n",
    "\"\"\"\n",
    "\n",
    "modelD317_returns = []\n",
    "for upto in tqdm_notebook(range(1500, 2100, 100)):\n",
    "\n",
    "    opt_params = bayes_opt_modelD(upto, ticks_to_use=317)\n",
    "    modelD317_returns.append(check_params(opt_params, upto, stock_num=317))\n",
    "\n",
    "modelD317_ret = get_est_series(pd.concat(modelD317_returns))\n",
    "\n",
    "print(\"Average return for Model D317:\", \\\n",
    "      round(modelD317_ret.iloc[-1]-100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf_D317 = pd.concat(modelD317_returns)\n",
    "\"\"\"\n",
    "Reading indices file. MDY ETF price must be there\n",
    "\"\"\"\n",
    "inddf = pd.read_excel('indices.xlsx', index_col=\"Date\")\n",
    "compare = pd.DataFrame()\n",
    "compare['No commissions'] = get_est_series(bigdf_D317, 0)\n",
    "compare['0.005 commissions'] = get_est_series(bigdf_D317, 0.005)\n",
    "compare['MDY ETF'] = inddf['mdy']\n",
    "compare['MDY ETF'] = compare['MDY ETF']/compare['MDY ETF'].iloc[0]*100\n",
    "compare.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_est_series(dtf, com=0):\n",
    "    \"\"\"\n",
    "    Calculates total return over several out-of-cross-validation samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - dataframe with 'avg' column\n",
    "    com=0 - value of commissions for ==one== side of a trade in USD/share\n",
    "    \n",
    "    Returns:\n",
    "    portf_series - series with portfolio value at a point in time\n",
    "    \"\"\"\n",
    "    \n",
    "    comdf = dtf.drop(columns=['avg'])\n",
    "    for tick in dtf.columns[:-1]:\n",
    "        if df[tick].iloc[1600] >= 0:\n",
    "            comdf[tick] = (new_data[tick].shift(-5)-com) / (new_data[tick]+com)-1\n",
    "            comdf[tick] = np.where(dtf[tick].notna(), comdf[tick], np.nan)\n",
    "        else:\n",
    "            comdf[tick] = np.nan\n",
    "    comdf['avg'] = comdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    portf_series = []\n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in comdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        portf_series.append(subportf.copy())\n",
    "        x+=1\n",
    "    \n",
    "    portf_series = pd.DataFrame(portf_series, index=comdf.index)\n",
    "    portf_series = portf_series.shift(5)\n",
    "    portf_series = portf_series.fillna(20).sum(axis=1)\n",
    "    \n",
    "    return portf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating new data.\n",
    "\"\"\"\n",
    "\n",
    "new_data = pd.read_excel('C:/Users/FS_Askar_A/long indices/midcapl.xlsx', index_col=\"Date\")\n",
    "\n",
    "new_dfs = {}\n",
    "\n",
    "for tick in tqdm_notebook(new_data.columns):\n",
    "    new_dfs[tick] = create_returns(new_data, tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This does one round of cross-validation to get optimized hyperparameters that worked for old data. \n",
    "They are then going to be used for model that predicts on new data.\n",
    "\"\"\"\n",
    "final_old_data = tickers_dfs[df.columns[0]].shape[0]\n",
    "opt_params = bayes_opt_modelD(final_old_data, ticks_to_use=317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = integerize(opt_params)\n",
    "bigdf = pd.DataFrame()\n",
    "\n",
    "for tick in tqdm_notebook(new_data.columns):\n",
    "\n",
    "    tempdf = new_dfs[tick][['fwd']]\n",
    "    trainx, testx, trainy, testy = my_tss(new_dfs[tick], final_old_data, train_size=params['train_size'])\n",
    "    \n",
    "    if testx.shape[0] > 0:\n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "\n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "\n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "\n",
    "bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "\n",
    "inddf = pd.read_excel('indices.xlsx', index_col=\"Date\")\n",
    "compare = pd.DataFrame()\n",
    "compare['No commissions'] = new_est_series(bigdf, 0)\n",
    "compare['0.005 commissions'] = new_est_series(bigdf, 0.005)\n",
    "compare['MDY ETF'] = inddf['mdy']\n",
    "compare['MDY ETF'] = compare['MDY ETF']/compare['MDY ETF'].iloc[0]*100\n",
    "compare.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
