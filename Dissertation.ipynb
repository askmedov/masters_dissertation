{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import matthews_corrcoef as matt\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading the data. Data has stock tickers in the first row and dates in the first column. Only trading days are used.\n",
    "\"\"\"\n",
    "df = pd.read_excel('midcap.xlsx', index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following methods are used throughout all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_returns(dtf, tick, lead=5, lags=[1,2,3,4,5,7,10,15,20,30,50], tr=0.025):\n",
    "    \"\"\"\n",
    "    Calculates forward and lagged returns for a stock. Also creates target column\n",
    "    for returns over some threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - data with all stock returns\n",
    "    tick - ticker of stock \n",
    "    lead - forward return lead\n",
    "    lags - lagged returns\n",
    "    tr - threshold value for target variable\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame for one stock with its price, forward and lagged returns and target column.\n",
    "    \"\"\"\n",
    "    dtf = dtf[[tick]]\n",
    "    dtf['fwd'] = dtf[tick].shift(-lead) / dtf[tick]-1\n",
    "    for lag in lags:\n",
    "        name = 'ret'+str(lag)\n",
    "        dtf[name] = dtf[tick]/dtf[tick].shift(lag)-1\n",
    "    dtf['ycol'] = np.where(dtf['fwd'] >= tr, 1, 0)\n",
    "    return dtf.dropna()\n",
    "\n",
    "\n",
    "def my_tss(dtf, split, train_size=1000, test_size=100):\n",
    "    \"\"\"\n",
    "    My take on time series split. It separates dataframe into \"past\" and \n",
    "    \"future\" from a splitting point. Also removes extra columns. \n",
    "    \n",
    "    Parameters:\n",
    "    dtf - dataframe of a stock\n",
    "    split - splitting point in trading days\n",
    "    train_size=1000 - size of the training sample\n",
    "    test_size=100 - size of the testing sample\n",
    "    \n",
    "    Returns:\n",
    "    trainx - training features\n",
    "    trainy - training target column\n",
    "    testx - testing features\n",
    "    testy - testing target column\n",
    "    \n",
    "    Attention!\n",
    "    This function is not fool-proof - it doesn't check whether the size of \n",
    "    the train size is less than splitting point or if testing sample has\n",
    "    any values. This is done for speed purposes as it is called thousands of times.\n",
    "    \"\"\"\n",
    "    train_start = split - train_size\n",
    "    test_end = split + test_size\n",
    "    trainx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol']).iloc[train_start:split]\n",
    "    testx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol']).iloc[split:test_end]\n",
    "    trainy = dtf['ycol'].iloc[train_start:split]\n",
    "    testy = dtf['ycol'].iloc[split:test_end]\n",
    "    return trainx, testx, trainy, testy\n",
    "\n",
    "\n",
    "def integerize(d):\n",
    "    \"\"\"\n",
    "    Converts hyperparameter values into integers. This is a compensation of hyperopt's \n",
    "    problem where it feeds integer values in float form. I.e. 2.0 instead of 2.\n",
    "    \n",
    "    Parameters:\n",
    "    d - dictionary of hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary of hyperparameters with integers where required\n",
    "    \"\"\"\n",
    "    \n",
    "    int_params = [\n",
    "        'train_size',\n",
    "        'test_size',\n",
    "        'num_leaves',\n",
    "        'max_depth',\n",
    "        'n_estimators',\n",
    "        'min_child_samples',\n",
    "        'upto'\n",
    "    ]\n",
    "    \n",
    "    for k in d:\n",
    "        if k in int_params:\n",
    "            d[k] = int(d[k])\n",
    "            \n",
    "    return d\n",
    "\n",
    "\n",
    "def calc_ret(results):\n",
    "    \"\"\"\n",
    "    Calculation of return from predicted data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results - list of dataframes. Dataframes must have 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ===negative=== return for the whole period, across all the dataframes. \n",
    "                Return is negative because hyperopt minimizes a function.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigdf = pd.concat(results)\n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in bigdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        x+=1\n",
    "        \n",
    "    return 1-np.sum(subportf)/portf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating returns for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86edd6ea3e94a1a8b133581f0262032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=317), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tickers_dfs = {}\n",
    "\n",
    "for tick in tqdm_notebook(df.columns):\n",
    "    tickers_dfs[tick] = create_returns(df, tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173bb5ec0d1b4b65a36661d39a32b389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average matt score across all stocks: 0.022\n",
      "Min matt score across all stocks: -0.007\n",
      "Max matt score across all stocks: 0.063\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model A317 - all 317 stocks\n",
    "\"\"\"\n",
    "\n",
    "modelA317_results = {}\n",
    "\n",
    "for tick in tqdm_notebook(df.columns):\n",
    "    \n",
    "    tick_results = []\n",
    "    \n",
    "    for split in range(1000, 2200, 100):\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split)\n",
    "        my_model = LGBMClassifier()\n",
    "        my_model.fit(trainx, trainy)\n",
    "        predy = my_model.predict(testx)\n",
    "        tick_results.append(matt(testy, predy))\n",
    "    \n",
    "    modelA317_results[tick] = np.mean(tick_results)\n",
    "\n",
    "modelA317_results = pd.Series(modelA317_results)    \n",
    "\n",
    "print(\"Average matt score across all stocks:\", round(modelA317_results.mean(), 3))\n",
    "print(\"Min matt score across all stocks:\", round(modelA317_results.min(), 3))\n",
    "print(\"Max matt score across all stocks:\", round(modelA317_results.max(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model A20 - first 20 stocks\n",
    "\"\"\"\n",
    "\n",
    "modelA20_results = {}\n",
    "\n",
    "for tick in tqdm_notebook(df.columns[:20]):\n",
    "    \n",
    "    tick_results = []\n",
    "    \n",
    "    for split in range(1000, 2200, 100):\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split)\n",
    "        my_model = LGBMClassifier()\n",
    "        my_model.fit(trainx, trainy)\n",
    "        predy = my_model.predict(testx)\n",
    "        tick_results.append(matt(testy, predy))\n",
    "    \n",
    "    modelA20_results[tick] = np.mean(tick_results)\n",
    "\n",
    "modelA20_results = pd.Series(modelA20_results)    \n",
    "\n",
    "print(\"Average matt score for the first 20 stocks:\", round(modelA20_results.mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to hyperparameters change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter num_leaves changed value to 4 Average matt score changed to 0.044\n",
      "Parameter max_depth changed value to 5 Average matt score changed to 0.015\n",
      "Parameter learning_rate changed value to 0.01 Average matt score changed to 0.004\n",
      "Parameter n_estimators changed value to 20 Average matt score changed to 0.014\n",
      "Parameter min_child_samples changed value to 10 Average matt score changed to 0.013\n"
     ]
    }
   ],
   "source": [
    "def get_res(params):\n",
    "    \n",
    "    mymodel_results = []\n",
    "\n",
    "    for tick in df.columns:\n",
    "\n",
    "        for split in range(1000, 2200, 100):\n",
    "            trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split)\n",
    "            my_model = LGBMClassifier(**params)\n",
    "            my_model.fit(trainx, trainy)\n",
    "            predy = my_model.predict(testx)\n",
    "            mymodel_results.append(matt(testy, predy))\n",
    "\n",
    "    return np.mean(mymodel_results)\n",
    "\n",
    "params_changes = {\n",
    "    'num_leaves': 4,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 20,\n",
    "    'min_child_samples': 10\n",
    "}\n",
    "\n",
    "for k, v in params_changes.items():\n",
    "    print(\"Parameter\", k, \"changed value to\", v, \\\n",
    "          \"Average matt score changed to\", round(get_res({k: v}), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_upto_modelB(params):\n",
    "    \"\"\"\n",
    "    Cross-validation function that takes does hyperparameters \n",
    "    validation upto some maximum split point and yields average \n",
    "    matt score result as output (negative for hyperopt purposes)\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    params - dictionary with hyperparameters from hyperopt\n",
    "                and 'upto' key, that limits cross-validation in time\n",
    "                \n",
    "    Returns:\n",
    "    --------\n",
    "    average matt score for the cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    params = integerize(params)\n",
    "    results = []\n",
    "    \n",
    "    my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                         max_depth=params['max_depth'],\n",
    "                                         learning_rate=params['learning_rate'],\n",
    "                                         n_estimators=params['n_estimators'],\n",
    "                                         min_child_samples=params['min_child_samples'])\n",
    "    \n",
    "    for split in range(params['upto']-500, params['upto'], 100):\n",
    "        \n",
    "        for tick in df.columns[:20]:\n",
    "            \n",
    "            trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split)\n",
    "            \n",
    "            my_model.fit(trainx, trainy)\n",
    "            predy = my_model.predict(testx)\n",
    "            results.append(matt(testy, predy))\n",
    "    \n",
    "    return -np.mean(results)\n",
    "\n",
    "def bayes_opt_modelB(upto, eval_n=100):\n",
    "    \"\"\"\n",
    "    Hyperopt's Bayesian optimization.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    upto - maximum split point for cross-validations\n",
    "    eval_n=100 - number of evaluations to do\n",
    "    \n",
    "    Returns:\n",
    "    tpe_best - best found hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    space_index = {\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 50, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 3, 8, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.05), np.log(0.2)),\n",
    "        'n_estimators': hp.quniform('n_estimators', 32, 512, 8),\n",
    "        'min_child_samples': hp.quniform('min_child_samples', 10, 50, 5),\n",
    "        'upto': hp.choice('upto', [upto])\n",
    "    }\n",
    "    \n",
    "    tpe_algo = tpe.suggest\n",
    "    tpe_trials = Trials()\n",
    "    tpe_best = fmin(fn=analyze_upto_modelB, space=space_index, algo=tpe_algo, \n",
    "                    trials=tpe_trials, max_evals=eval_n, verbose=True)\n",
    "    \n",
    "    return tpe_best\n",
    "\n",
    "def predict_params_modelB(params, upto):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters. \n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    params - optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    average matt score result\n",
    "    \"\"\"\n",
    "    \n",
    "    params = integerize(params)\n",
    "    results = []\n",
    "    \n",
    "    for tick in df.columns[:20]:\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], upto+100)\n",
    "        \n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "                    \n",
    "        my_model.fit(trainx, trainy)\n",
    "        predy = my_model.predict(testx)\n",
    "        results.append(matt(testy, predy))\n",
    "        \n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6982b8823ba84f6483b21c1f6322c2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculating the overall matt score for Model B20\n",
    "First 20 stocks only.\n",
    "\"\"\"\n",
    "\n",
    "modelB20_results = []\n",
    "for upto in tqdm_notebook(range(1500, 2100, 100)):\n",
    "\n",
    "    opt_params = bayes_opt_modelB(upto)\n",
    "    modelB20_results.append(predict_params_modelB(opt_params, upto))\n",
    "    \n",
    "print(\"Average score for Model B20:\", \\\n",
    "      round(np.mean(modelB20_results), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_upto_modelC(params):\n",
    "    \"\"\"\n",
    "    Cross-validation function that takes does hyperparameters \n",
    "    validation upto some maximum split point and yields average \n",
    "    matt score result as output (negative for hyperopt purposes)\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    params - dictionary with hyperparameters from hyperopt\n",
    "                and 'upto' key, that limits cross-validation in time\n",
    "                \n",
    "    Returns:\n",
    "    --------\n",
    "    average matt score for the cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    params = integerize(params)\n",
    "    results = []\n",
    "    \n",
    "    my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                         max_depth=params['max_depth'],\n",
    "                                         learning_rate=params['learning_rate'],\n",
    "                                         n_estimators=params['n_estimators'],\n",
    "                                         min_child_samples=params['min_child_samples'])\n",
    "    \n",
    "    for split in range(params['upto']-500, params['upto'], 100):\n",
    "        \n",
    "        for tick in df.columns[:20]:\n",
    "            \n",
    "            trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split, \n",
    "                                                      train_size=params['train_size'])\n",
    "            \n",
    "            sw = np.where(trainy==0, params['sw'], 1)\n",
    "            \n",
    "            my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "            predy = my_model.predict(testx)\n",
    "            results.append(matt(testy, predy))\n",
    "    \n",
    "    return -np.mean(results)\n",
    "\n",
    "def bayes_opt_modelC(upto, eval_n=100):\n",
    "    \"\"\"\n",
    "    Hyperopt's Bayesian optimization.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    upto - maximum split point for cross-validations\n",
    "    eval_n=100 - number of evaluations to do\n",
    "    \n",
    "    Returns:\n",
    "    tpe_best - best found hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    space_index = {\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 50, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 3, 8, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.05), np.log(0.2)),\n",
    "        'n_estimators': hp.quniform('n_estimators', 32, 512, 8),\n",
    "        'min_child_samples': hp.quniform('min_child_samples', 10, 50, 5),\n",
    "        'sw': hp.uniform('sw', 0.4, 0.7),\n",
    "        'train_size': hp.quniform('train_size', 400, 1000, 100),\n",
    "        'upto': hp.choice('upto', [upto])\n",
    "    }\n",
    "    \n",
    "    tpe_algo = tpe.suggest\n",
    "    tpe_trials = Trials()\n",
    "    tpe_best = fmin(fn=analyze_upto_modelC, space=space_index, algo=tpe_algo, \n",
    "                    trials=tpe_trials, max_evals=eval_n, verbose=True)\n",
    "    \n",
    "    return tpe_best\n",
    "\n",
    "def predict_params_modelC(params, upto):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters. \n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    params - optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    average matt score result\n",
    "    \"\"\"\n",
    "    \n",
    "    params = integerize(params)\n",
    "    results = []\n",
    "    \n",
    "    for tick in df.columns[:20]:\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        \n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "        \n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split, \n",
    "                                                      train_size=params['train_size'])\n",
    "            \n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "\n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        predy = my_model.predict(testx)\n",
    "        results.append(matt(testy, predy))\n",
    "        \n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_params(params, upto, stock_num=20):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters. \n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    params - optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe with 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \"\"\"\n",
    "    params = integerize(params)\n",
    "    bigdf = pd.DataFrame()\n",
    "    \n",
    "    for tick in df.columns[:stock_num]:\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], upto+100, \n",
    "                                              train_size=params['train_size'])\n",
    "        \n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "        \n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "            \n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        \n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "        \n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "        \n",
    "    return bigdf\n",
    "\n",
    "def get_est_series(dtf, com=0):\n",
    "    \"\"\"\n",
    "    Calculates total return over several out-of-cross-validation samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - dataframe with 'avg' column\n",
    "    com=0 - value of commissions for ==one== side of a trade in USD/share\n",
    "    \n",
    "    Returns:\n",
    "    portf_series - series with portfolio value at a point in time\n",
    "    \"\"\"\n",
    "    \n",
    "    comdf = dtf.drop(columns=['avg'])\n",
    "    for tick in bigdf.columns[:-1]:\n",
    "        if tdf[tick].iloc[1600] >= 0:\n",
    "            comdf[tick] = (tdf[tick].shift(-5)-com) / (tdf[tick]+com)-1\n",
    "            comdf[tick] = np.where(bigdf[tick].notna(), comdf[tick], np.nan)\n",
    "        else:\n",
    "            comdf[tick] = np.nan\n",
    "    comdf['avg'] = comdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    portf_series = []\n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in comdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        portf_series.append(subportf.copy())\n",
    "        x+=1\n",
    "    \n",
    "    portf_series = pd.DataFrame(portf_series, index=comdf.index)\n",
    "    portf_series = portf_series.shift(5)\n",
    "    portf_series = portf_series.fillna(20).sum(axis=1)\n",
    "    \n",
    "    return portf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3740512e1a414f7cbee7dc6bc74ad669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculating the overall matt score for Model C20\n",
    "First 20 stocks only.\n",
    "\"\"\"\n",
    "\n",
    "modelC20_matt = []\n",
    "for upto in tqdm_notebook(range(1500, 2100, 100)):\n",
    "\n",
    "    opt_params = bayes_opt_modelC(upto, eval_n=3)\n",
    "    modelC20_matt.append(predict_params_modelC(opt_params, upto))\n",
    "    \n",
    "print(\"Average score for Model C20:\", \\\n",
    "      round(np.mean(modelC20_matt), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "bayes_step = 1 \n",
    "\n",
    "def analyze_upto(params):\n",
    "    global bayes_step\n",
    "    t0 = time()\n",
    "    \n",
    "    params = integerize(params)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                         max_depth=params['max_depth'],\n",
    "                                         learning_rate=params['learning_rate'],\n",
    "                                         n_estimators=params['n_estimators'],\n",
    "                                         min_child_samples=params['min_child_samples'])\n",
    "    \n",
    "    for split in range(params['upto']-500, params['upto'], 100):\n",
    "        \n",
    "        tempdf = pd.DataFrame()\n",
    "        \n",
    "        for tick in df.columns:\n",
    "            trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split, \n",
    "                                                      train_size=params['train_size'], \n",
    "                                                      test_size=100)\n",
    "            \n",
    "            sw = np.where(trainy==0, params['sw'], 1)\n",
    "            \n",
    "            my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "            testx['pred'] = my_model.predict(testx)\n",
    "            tempdf['pred'] = testx['pred']\n",
    "            tempdf['fwd'] = tickers_dfs[tick]['fwd']\n",
    "            tempdf[tick] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        \n",
    "        results.append(tempdf)\n",
    "    \n",
    "    res = calc_ret(results)\n",
    "    print(\"Step #:\", bayes_step, round(-res, 4), 'Time elapsed: '+str(round(time()-t0,2)))\n",
    "    bayes_step+=1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_opt(upto, eval_n=100):\n",
    "    space_index = {\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 50, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 3, 8, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.05), np.log(0.2)),\n",
    "        'n_estimators': hp.quniform('n_estimators', 32, 512, 8),\n",
    "        'min_child_samples': hp.quniform('min_child_samples', 10, 50, 5),\n",
    "        'sw': hp.uniform('sw', 0.4, 0.7),\n",
    "        'train_size': hp.quniform('train_size', 400, 1000, 100),\n",
    "        'upto': hp.choice('upto', [upto])\n",
    "    }\n",
    "    tpe_algo = tpe.suggest\n",
    "    tpe_trials = Trials()\n",
    "    tpe_best = fmin(fn=analyze_upto, space=space_index, algo=tpe_algo, trials=tpe_trials, \n",
    "                    max_evals=eval_n, verbose=True)\n",
    "    return tpe_best, tpe_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_params(params, upto):\n",
    "    \n",
    "    params = integerize(params)\n",
    "    bigdf = pd.DataFrame()\n",
    "    \n",
    "    for tick in df.columns:\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], upto+100, \n",
    "                                              train_size=params['train_size'], \n",
    "                                                  test_size=100)\n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "        \n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "            \n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "        \n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "        \n",
    "    return bigdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_excel('long indices/midcapl.xlsx', index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tickers_tdfs = {}\n",
    "\n",
    "for tick in tqdm_notebook(tdf.columns):\n",
    "    tickers_tdfs[tick] = create_returns(tdf, tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_dfs['exel'].shape, tickers_tdfs['exel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_step = 1 \n",
    "opt_params, opt_trials = bayes_opt(tickers_dfs['exel'].shape[0], eval_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frombest = 1\n",
    "opt_params = pd.DataFrame(opt_trails1).T.sort_values(0).iloc[frombest][1]\n",
    "print(pd.DataFrame(opt_trails1).T.sort_values(0).iloc[frombest][0])\n",
    "params = integerize(opt_params)\n",
    "bigdf = pd.DataFrame()\n",
    "\n",
    "for tick in tqdm_notebook(tdf.columns):\n",
    "\n",
    "    tempdf = tickers_tdfs[tick][['fwd']]\n",
    "    trainx, testx, trainy, testy = my_tss(tickers_tdfs[tick], tickers_dfs['exel'].shape[0], \n",
    "                                          train_size=params['train_size'], \n",
    "                                              test_size=100)\n",
    "    if testx.shape[0] > 0:\n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "\n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "\n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "\n",
    "bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inddf = pd.read_excel('indices.xlsx', index_col=\"Date\")\n",
    "compare = pd.DataFrame()\n",
    "compare['No commissions'] = get_est_series(bigdf, 0)\n",
    "compare['0.005 commissions'] = get_est_series(bigdf, 0.005)\n",
    "compare['MDY ETF'] = inddf['mdy'].shift(-0)\n",
    "compare['MDY ETF'] = compare['MDY ETF']/compare['MDY ETF'].iloc[0]*100\n",
    "compare.to_excel(\"model5OST.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trials(trial_dict):\n",
    "    out_dict = {}\n",
    "    for k, v in trial_dict.items():\n",
    "        out_dict[k] = v[0]\n",
    "        \n",
    "    return out_dict\n",
    "\n",
    "opt_trails1 = {}\n",
    "for x in range(100):\n",
    "    opt_trails1[x] = [opt_trials.trials[x]['result']['loss'], convert_trials(opt_trials.trials[x]['misc']['vals'])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(opt_trails1).T.sort_values(0).iloc[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allres = []\n",
    "for upto in tqdm_notebook(range(1500, 2100, 100)):\n",
    "\n",
    "    opt_params = bayes_opt(upto, eval_n=100)\n",
    "    allres.append(check_params(opt_params, upto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdf = pd.read_excel(\"model5return317.xlsx\", index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_est_series(dtf, com=0):\n",
    "\n",
    "    comdf = dtf.drop(columns=['avg'])\n",
    "    for tick in bigdf.columns[:-1]:\n",
    "        if tdf[tick].iloc[1600] >= 0:\n",
    "            comdf[tick] = (tdf[tick].shift(-5)-com) / (tdf[tick]+com)-1\n",
    "            comdf[tick] = np.where(bigdf[tick].notna(), comdf[tick], np.nan)\n",
    "        else:\n",
    "            comdf[tick] = np.nan\n",
    "    comdf['avg'] = comdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    portf_series = []\n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in comdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        portf_series.append(subportf.copy())\n",
    "        x+=1\n",
    "    \n",
    "    portf_series = pd.DataFrame(portf_series, index=comdf.index)\n",
    "    portf_series = portf_series.shift(5)\n",
    "    portf_series = portf_series.fillna(20).sum(axis=1)\n",
    "    \n",
    "    return portf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inddf = pd.read_excel('indices.xlsx', index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.DataFrame()\n",
    "compare['No commissions'] = get_est_series(bigdf, 0)\n",
    "compare['0.005 commissions'] = get_est_series(bigdf, 0.005)\n",
    "compare['B-E commissions'] = get_est_series(bigdf, 0.023)\n",
    "compare['MDY ETF'] = inddf['mdy'].shift(-0)\n",
    "compare['MDY ETF'] = compare['MDY ETF']/compare['MDY ETF'].iloc[0]*100\n",
    "compare.to_excel(\"model5compare317return.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare['B-E commissions'] = get_est_series(bigdf, 0.024)\n",
    "compare['B-E commissions'].iloc[-1]/compare['MDY ETF'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(compare.iloc[-1]/100)**(252/600)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf.count(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdf.to_excel('model5return317.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf.to_excel('model5return20.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
