{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import matthews_corrcoef as matt\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading the data. Data has stock tickers in the first row and dates in the first column. Only trading days are used.\n",
    "\"\"\"\n",
    "df = pd.read_excel('C:/Users/FS_Askar_A/midcap.xlsx', index_col=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_returns(dtf, tick, lead=5, lags=[1,2,3,4,5,7,10,15,20,30,50], tr=0.025):\n",
    "    \"\"\"\n",
    "    Calculates forward and lagged returns for a stock. Also creates target column\n",
    "    for returns over some threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - data with all stock returns\n",
    "    tick - ticker of stock \n",
    "    lead - forward return lead\n",
    "    lags - lagged returns\n",
    "    tr - threshold value for target variable\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame for one stock with its price, forward and lagged returns and target column.\n",
    "    \"\"\"\n",
    "    dtf = dtf[[tick]]\n",
    "    dtf['fwd'] = dtf[tick].shift(-lead) / dtf[tick]-1\n",
    "    for lag in lags:\n",
    "        name = 'ret'+str(lag)\n",
    "        dtf[name] = dtf[tick]/dtf[tick].shift(lag)-1\n",
    "    dtf['ycol'] = np.where(dtf['fwd'] >= tr, 1, 0)\n",
    "    return dtf.dropna()\n",
    "\n",
    "\n",
    "def my_tss(dtf, split, train_size=1000, test_size=100):\n",
    "    \"\"\"\n",
    "    My take on time series split. It separates dataframe into \"past\" and \n",
    "    \"future\" from a splitting point. Also removes extra columns. \n",
    "    \n",
    "    Parameters:\n",
    "    dtf - dataframe of a stock\n",
    "    split - splitting point in trading days\n",
    "    train_size=1000 - size of the training sample\n",
    "    test_size=100 - size of the testing sample\n",
    "    \n",
    "    Returns:\n",
    "    trainx - training features\n",
    "    trainy - training target column\n",
    "    testx - testing features\n",
    "    testy - testing target column\n",
    "    \n",
    "    Attention!\n",
    "    This function is not fool-proof - it doesn't check whether the size of \n",
    "    the train size is less than splitting point or if testing sample has\n",
    "    any values. This is done for speed purposes as it is called thousands of times.\n",
    "    \"\"\"\n",
    "    train_start = split - train_size\n",
    "    test_end = split + test_size\n",
    "    trainx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol']).iloc[train_start:split]\n",
    "    testx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol']).iloc[split:test_end]\n",
    "    trainy = dtf['ycol'].iloc[train_start:split]\n",
    "    testy = dtf['ycol'].iloc[split:test_end]\n",
    "    return trainx, testx, trainy, testy\n",
    "\n",
    "\n",
    "def integerize(d):\n",
    "    \"\"\"\n",
    "    Converts hyperparameter values into integers. This is a compensation of hyperopt's \n",
    "    problem where it feeds integer values in float form. I.e. 2.0 instead of 2.\n",
    "    \n",
    "    Parameters:\n",
    "    d - dictionary of hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary of hyperparameters with integers where required\n",
    "    \"\"\"\n",
    "    \n",
    "    int_params = [\n",
    "        'train_size',\n",
    "        'test_size',\n",
    "        'num_leaves',\n",
    "        'max_depth',\n",
    "        'n_estimators',\n",
    "        'min_child_samples',\n",
    "        'upto',\n",
    "        'ticks_to_use'\n",
    "    ]\n",
    "    \n",
    "    for k in d:\n",
    "        if k in int_params:\n",
    "            d[k] = int(d[k])\n",
    "            \n",
    "    return d\n",
    "\n",
    "\n",
    "def calc_ret(results):\n",
    "    \"\"\"\n",
    "    Calculation of return from predicted data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results - list of dataframes. Dataframes must have 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ===negative=== return for the whole period, across all the dataframes. \n",
    "                Return is negative because hyperopt minimizes a function.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigdf = pd.concat(results)\n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in bigdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        x+=1\n",
    "        \n",
    "    return 1-np.sum(subportf)/portf\n",
    "\n",
    "\n",
    "def check_params(params, upto, stock_num=20):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters. \n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    params - optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe with 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \"\"\"\n",
    "    params = integerize(params)\n",
    "    bigdf = pd.DataFrame()\n",
    "    \n",
    "    for tick in df.columns[:stock_num]:\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], upto+100, \n",
    "                                              train_size=params['train_size'])\n",
    "        \n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "        \n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "            \n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        \n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "        \n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "        \n",
    "    return bigdf\n",
    "\n",
    "\n",
    "def check_paramsEF(tick_params, upto):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters for every ticker separately. \n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    tick_params - dict of optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe with 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigdf = pd.DataFrame()\n",
    "    \n",
    "    for tick, params in tick_params.items():\n",
    "        \n",
    "        params = integerize(params)\n",
    "        \n",
    "        tempdf = tickers_dfs[tick][['fwd']]\n",
    "        trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], upto+100, \n",
    "                                              train_size=params['train_size'])\n",
    "        \n",
    "        my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                     max_depth=params['max_depth'],\n",
    "                                     learning_rate=params['learning_rate'],\n",
    "                                     n_estimators=params['n_estimators'],\n",
    "                                     min_child_samples=params['min_child_samples'])\n",
    "        \n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "\n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        \n",
    "        testx['pred'] = my_model.predict(testx)\n",
    "        tempdf['pred'] = testx['pred']\n",
    "        tempdf.dropna(inplace=True)\n",
    "        tempdf['predret'] = np.where(tempdf['pred']==1, tempdf['fwd'], np.nan)\n",
    "        bigdf[tick] = tempdf['predret']\n",
    "        \n",
    "    bigdf['avg'] = bigdf.mean(axis=1).fillna(0)+1\n",
    "        \n",
    "    return bigdf\n",
    "\n",
    "\n",
    "def get_est_series(dtf, com=0):\n",
    "    \"\"\"\n",
    "    Calculates total return over several out-of-cross-validation samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - dataframe with 'avg' column\n",
    "    com=0 - value of commissions for ==one== side of a trade in USD/share\n",
    "    \n",
    "    Returns:\n",
    "    portf_series - series with portfolio value at a point in time\n",
    "    \"\"\"\n",
    "    \n",
    "    comdf = dtf.drop(columns=['avg'])\n",
    "    for tick in dtf.columns[:-1]:\n",
    "        if df[tick].iloc[1600] >= 0:\n",
    "            comdf[tick] = (df[tick].shift(-5)-com) / (df[tick]+com)-1\n",
    "            comdf[tick] = np.where(dtf[tick].notna(), comdf[tick], np.nan)\n",
    "        else:\n",
    "            comdf[tick] = np.nan\n",
    "    comdf['avg'] = comdf.mean(axis=1).fillna(0)+1\n",
    "    \n",
    "    portf_series = []\n",
    "    lead = 5\n",
    "    portf = 100\n",
    "    subportf = [portf/lead for l in range(lead)]\n",
    "    x = 0\n",
    "    for day in comdf['avg']:\n",
    "        subportf[x%5] = subportf[x%5]*day\n",
    "        portf_series.append(subportf.copy())\n",
    "        x+=1\n",
    "    \n",
    "    portf_series = pd.DataFrame(portf_series, index=comdf.index)\n",
    "    portf_series = portf_series.shift(5)\n",
    "    portf_series = portf_series.fillna(20).sum(axis=1)\n",
    "    \n",
    "    return portf_series\n",
    "\n",
    "\n",
    "def get_res(params):\n",
    "    \n",
    "    mymodel_results = []\n",
    "\n",
    "    for tick in df.columns:\n",
    "\n",
    "        for split in range(1000, 2200, 100):\n",
    "            trainx, testx, trainy, testy = my_tss(tickers_dfs[tick], split)\n",
    "            my_model = LGBMClassifier(**params)\n",
    "            my_model.fit(trainx, trainy)\n",
    "            predy = my_model.predict(testx)\n",
    "            mymodel_results.append(matt(testy, predy))\n",
    "\n",
    "    return np.mean(mymodel_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tickers_dfs = {}\n",
    "\n",
    "for tick in tqdm_notebook(df.columns):\n",
    "    tickers_dfs[tick] = create_returns(df, tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_returns_GH(dtf, tick, lead=5, lags=[1,2,3,4,5,7,10,15,20,30,50], tr=0.025):\n",
    "    \"\"\"\n",
    "    Calculates forward and lagged returns for a stock. \n",
    "    Creates target column for returns over some threshold. \n",
    "    Calculates standard deviation (rolled 1-year).\n",
    "    Converts returns into standard deviations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtf - data with all stock returns\n",
    "    tick - ticker of stock \n",
    "    lead - forward return lead\n",
    "    lags - lagged returns\n",
    "    tr - threshold value for target variable\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame for one stock with its forward and \n",
    "    lagged returns in standard deviations and target column.\n",
    "    \"\"\"\n",
    "    \n",
    "    dtf = dtf[[tick]]\n",
    "    dtf['fwd'] = dtf[tick].shift(-lead) / dtf[tick]-1\n",
    "    for lag in lags:\n",
    "        name = 'ret'+str(lag)\n",
    "        dtf[name] = dtf[tick]/dtf[tick].shift(lag)-1\n",
    "    dtf['ycol'] = np.where(dtf['fwd'] >= tr, 1, 0)\n",
    "    dtf['std'] = dtf['ret1'].rolling(252).std()\n",
    "    for c in dtf.columns:\n",
    "        if 'ret' in c:\n",
    "            dtf[c] = dtf[c]/dtf['std']\n",
    "    \n",
    "    dtf['tick'] = tick\n",
    "    \n",
    "    return dtf.drop(columns=[tick]).dropna()\n",
    "\n",
    "\"\"\"\n",
    "Creates new dataframes dictionary (only for models G and H)\n",
    "\"\"\"\n",
    "tickers_GH = {}\n",
    "\n",
    "for tick in tqdm_notebook(df.columns):\n",
    "    tickers_GH[tick] = create_returns_GH(df, tick)\n",
    "    \n",
    "\"\"\"\n",
    "Combines the dataframes together \n",
    "\"\"\"    \n",
    "training = pd.concat(tickers_GH.values())\n",
    "training.sort_index(inplace=True)\n",
    "training.index = training.index.astype(str) + \":\" + training['tick']\n",
    "\n",
    "csize = df.shape[1]\n",
    "\n",
    "def my_tss_GH(dtf, split, train_size=1000, test_size=100, chunk_size=csize):\n",
    "    \"\"\"\n",
    "    My take on time series split. It separates dataframe into \"past\" and \n",
    "    \"future\" from a splitting point. Also removes extra columns. \n",
    "    \n",
    "    Parameters:\n",
    "    dtf - dataframe of a stock\n",
    "    split - splitting point in trading days\n",
    "    train_size=1000 - size of the training sample\n",
    "    test_size=100 - size of the testing sample\n",
    "    chunk_size=csize - number of stocks in the dataframe (chunk size)\n",
    "    \n",
    "    Returns:\n",
    "    trainx - training features\n",
    "    trainy - training target column\n",
    "    testx - testing features\n",
    "    testy - testing target column\n",
    "    \n",
    "    Attention!\n",
    "    This function is not fool-proof - it doesn't check whether the size of \n",
    "    the train size is less than splitting point or if testing sample has\n",
    "    any values. This is done for speed purposes as it is called thousands of times.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_start = (split - train_size)*chunk_size\n",
    "    test_end = (split + test_size)*chunk_size\n",
    "    split = split*chunk_size\n",
    "    trainx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol', 'std', 'tick']).iloc[train_start:split]\n",
    "    testx = dtf.drop(columns=[dtf.columns[0], 'fwd', 'ycol', 'std', 'tick']).iloc[split:test_end]\n",
    "    trainy = dtf['ycol'].iloc[train_start:split]\n",
    "    testy = dtf['ycol'].iloc[split:test_end]\n",
    "    return trainx, testx, trainy, testy\n",
    "\n",
    "def get_preddf(testx):\n",
    "    \"\"\"\n",
    "    Pulls forward returns to predicted positive classes and then creates dataframe with 'avg' column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    \"\"\"\n",
    "    testx['date'] = pd.Series(testx.index).apply(lambda x: x.split(\":\")[0]).values\n",
    "    testx['tick'] = pd.Series(testx.index).apply(lambda x: x.split(\":\")[1]).values\n",
    "    testx['fwd'] = training['fwd']\n",
    "    testx['pred_ret'] = np.where(testx['pred'] == 1, testx['fwd'], np.nan)\n",
    "    tempdf = testx.pivot_table(values='pred_ret', index='date', columns='tick', dropna=False)\n",
    "    tempdf['avg'] = tempdf.mean(axis=1).fillna(0)+1\n",
    "    tempdf.index = pd.to_datetime(tempdf.index)\n",
    "    return tempdf\n",
    "\n",
    "\n",
    "def check_params_GH(params, upto):\n",
    "    \"\"\"\n",
    "    Predicts on out-of-sample and out-of-cross-validation data\n",
    "    using optimized hyperparameters for all stocks at once - this function i\n",
    "    s Model G and H specific\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    params - optimized hyperparameters\n",
    "    upto - maximum split point for cross-validations,\n",
    "            here it is incremented by 100 to not include the\n",
    "            last validation sample (100 points in size)\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe with 'avg' column, which is the \n",
    "                average 5-day forward return for the day.\n",
    "    \"\"\"\n",
    "    params = integerize(params)\n",
    "    \n",
    "    trainx, testx, trainy, testy = my_tss_GH(training, upto+100, train_size=params['train_size'])\n",
    "    \n",
    "    my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                 max_depth=params['max_depth'],\n",
    "                                 learning_rate=params['learning_rate'],\n",
    "                                 n_estimators=params['n_estimators'],\n",
    "                                 min_child_samples=params['min_child_samples'])\n",
    "    \n",
    "    sw = np.where(trainy==0, params['sw'], 1)\n",
    "\n",
    "    my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "    proby = my_model.predict_proba(testx)[:,1]\n",
    "    \n",
    "    testx['pred'] = np.where(proby > params['conf'], 1, 0)\n",
    "        \n",
    "    return get_preddf(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_upto_modelH(params):\n",
    "    #return\n",
    "    params = integerize(params)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    my_model = LGBMClassifier(num_leaves=params['num_leaves'],\n",
    "                                         max_depth=params['max_depth'],\n",
    "                                         learning_rate=params['learning_rate'],\n",
    "                                         n_estimators=params['n_estimators'],\n",
    "                                         min_child_samples=params['min_child_samples'])\n",
    "    \n",
    "    for split in range(params['upto']-500, params['upto'], 100):\n",
    "        \n",
    "        trainx, testx, trainy, testy = my_tss_GH(training, split, \n",
    "                                                  train_size=params['train_size'])\n",
    "\n",
    "        sw = np.where(trainy==0, params['sw'], 1)\n",
    "\n",
    "        my_model.fit(trainx, trainy, sample_weight=sw)\n",
    "        proby = my_model.predict_proba(testx)[:,1]\n",
    "        testx['pred'] = np.where(proby > params['conf'], 1, 0)\n",
    "        results.append(get_preddf(testx))\n",
    "    \n",
    "    return 1-get_est_series(pd.concat(results)).iloc[-1]/100\n",
    "\n",
    "def bayes_opt_modelH(upto, eval_n=100):\n",
    "    space_index = {\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 50, 5),\n",
    "        'max_depth': hp.quniform('max_depth', 3, 8, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.05), np.log(0.2)),\n",
    "        'n_estimators': hp.quniform('n_estimators', 32, 256, 8),\n",
    "        'min_child_samples': hp.quniform('min_child_samples', 100, 500, 5),\n",
    "        'sw': hp.uniform('sw', 0.4, 0.7),\n",
    "        'train_size': hp.quniform('train_size', 50, 750, 50),\n",
    "        'conf': hp.uniform('conf', 0.4, 0.7),\n",
    "        'upto': hp.choice('upto', [upto])\n",
    "    }\n",
    "    tpe_algo = tpe.suggest\n",
    "    tpe_trials = Trials()\n",
    "    tpe_best = fmin(fn=analyze_upto_modelH, space=space_index, algo=tpe_algo, trials=tpe_trials, \n",
    "                    max_evals=eval_n, verbose=True)\n",
    "    return tpe_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating the overall matt score for Model H317\n",
    "All 317 stocks.\n",
    "\"\"\"\n",
    "\n",
    "modelH317_returns = []\n",
    "for upto in tqdm_notebook(range(1298, 1898, 100)):\n",
    "\n",
    "    opt_params = bayes_opt_modelH(upto)\n",
    "    modelH317_returns.append(check_params_GH(opt_params, upto))\n",
    "\n",
    "modelH317_ret = get_est_series(pd.concat(modelH317_returns))\n",
    "\n",
    "print(\"Average return for Model H317:\", \\\n",
    "      round(modelH317_ret.iloc[-1]-100, 2), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
